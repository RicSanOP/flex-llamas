   80  ls .ssh/
   81  cat .ssh/tablet.pub
   82  cat .ssh/tablet.pub >> .ssh/authorized_keys 
   83  vim .ssh/authorized_keys 
   84  ls
   85  ps -aux | grep 42077
   86  ls
   87  vim /etc/ssh/sshd_config
   88  sudo vim /etc/ssh/sshd_config
   89  sudo systemctl restart sshd
   90  python3 -m aider
   91  a
   92  ls
   93  ls flex-llamas-fine-tuning/
   94  cat flex-llamas-fine-tuning/README.md 
   95  which aider
   96  tmux ls
   97  ls
   98  tmux ls
   99  uptime
  100  tmux new -s britannio
  101  ;s
  102  ls
  103  cd prototyping/
  104  ls
  105  aider
  106  ls
  107  git init
  108  ls
  109  cat "venv" >> .gitignore
  110  ls
  111  echo "venv" >> .gitignore
  112  git status
  113  echo ".ipynb_checkpoints" >> .gitignore
  114  echo ".aider*" >> .gitignore
  115  git status
  116  git add .
  117  git commit -m 'pre-aider'
  118  git status
  119  ls
  120  aider --model openai
  121  ls
  122  ls
  123  clear
  124  ssh-keygen -t ed25519 -C "GitHub from Nebius at Meta Llama Hackathon"
  125  cat .ssh/github_flexllamas.pub 
  126  ls
  127  cd flex-llamas-fine-tuning/
  128  ls
  129  git status
  130  cd ..
  131  cat flex-llamas-fine-tuning/README.md 
  132  rm flex-llamas-fine-tuning/ -r
  133  sudo rm flex-llamas-fine-tuning/ -r
  134  mkdir flex-llamas
  135  cd flex-llamas/
  136  vim README.md
  137  ls
  138  cat test.txt
  139  rm test.txt
  140  git init
  141  git status
  142  ls -a
  143  git add .
  144  git commit -m "first commit from Nebius VM"
  145  git status
  146  git log
  147  git remote add origin git@github.com:RicSanOP/flex-llamas-fine-tuning.git
  148  vim ~/.ssh/config
  149  ls ~/.ssh
  150  vim ~/.ssh/config
  151  git push -u origin master
  152  vim ~/.ssh/config
  153  git push -u origin master
  154  lsblk
  155  git log
  156  htop
  157  cd prototyping/
  158  ls
  159  source venv/bin/activate
  160  ls
  161  echo "source ~/prototyping/venv/bin/activate" >> source.sh
  162  ls
  163  rm source.sh 
  164  ls
  165  python3 -m pip install kagglehub
  166  sudo apt-get install tree
  167  python3 -m pip install pandas polars torch numpy matplotlib plotnine
  168  ls
  169  ls .ssh
  170  cd .ssh
  171  ls
  172  rm tablet.pub 
  173  ls
  174  cd ..
  175  ls
  176  ks
  177  ls
  178  cd prototyping/
  179  ls
  180  mkdir datasets
  181  ls
  182  cd datasets/
  183  ls
  184  vim manifest.yaml
  185  ls
  186  vim download.py
  187  ls
  188  cd ..
  189  ls
  190  tmux ls
  191  cd prototyping/
  192  ls
  193  source venv/bin/activate
  194  ls
  195  cd prototyping/
  196  ls
  197  find . -name activate
  198  ./venv/bin/activate
  199  chmod a+X ./venv/bin/activate
  200  chmod a+x ./venv/bin/activate
  201  ./venv/bin/activate
  202  ls
  203  which python3
  204  . ./venv/bin/activate
  205  which python3
  206  ls
  207  git clone git@github.com:cerebralvalley/compute-boilerplate.git
  208  clear
  209  ls
  210  cd compute-boilerplate/
  211  ls 
  212  pip3 install -r requirements
  213  ls
  214  pip3 install -r requirements.txt 
  215  python --version
  216  python3 --version
  217  python3 -m venv .
  218  ls a
  219  ls -la
  220  cat main.py
  221  cat .config.py.swp 
  222  cd ..
  223  ls
  224  rm -r compute-boilerplate/
  225  sudor rm -r compute-boilerplate/
  226  sudo rm -r compute-boilerplate/
  227  ls
  228  ls flex-llamas/
  229  ls compute-boilerplate/
  230  cd compute-boilerplate/
  231  ls
  232  cat requirements.txt 
  233  cat use.py 
  234  clear
  235  ls
  236  vim main.py 
  237  ls
  238  cd compute-boilerplate/
  239  ls
  240  cat utils
  241  cat utils.py 
  242  vim utils.py 
  243  exit
  244  source /home/nebius/compute-boilerplate/bin/activate
  245  cd compute-boilerplate/
  246  source bin/activate
  247  pip3 install -r requirements.txt 
  248  cd ..
  249  sudo rm compute-boilerplate/ -r
  250  git clone git@github.com:cerebralvalley/compute-boilerplate.git
  251  cd compute-boilerplate/
  252  ls -a
  253  mkdir venv
  254  python3 -m venv venv/
  255  source venv/bin/activate
  256  ls
  257  cat use.py
  258  pip3 install -r requirements.txt 
  259  python3 main.py
  260  cd prototyping/
  261  ls
  262  source venv/bin/activate
  263  python3 -m pip install dotenv llamastack
  264  python3 -m pip install dotenv
  265  python3 -m pip install setupwheels
  266  python3 -m pip install setupwheel
  267  python3 -m pip install wheel
  268  python3 -m pip install dotenv
  269  python3 -m pip install python3-dotenv
  270  python3 -m pip install python-dotenv
  271  python3 -m pip install dotenv llamastack
  272  python3 -m pip install python-dotenv llamastack
  273  python3 -m pip install python-dotenv llama-stack
  274  python3 -m pip install utils
  275  ls
  276  touch utils.py
  277  rm utils.py 
  278  ls
  279  touch llm_api/utils.py
  280  ls
  281  cd llm_api/
  282  ls
  283  exit
  284  ls
  285  cd compute-boilerplate/
  286  ls
  287  cat config.py 
  288  ls
  289  mkdir llama_api
  290  ls
  291  touch llama_api/__init__.py
  292  ls
  293  rm -rf llama_api/
  294  ls
  295  git status
  296  cd ..
  297  ls
  298  cd prototyping/
  299  ls
  300  mkdir llm_api
  301  ls
  302  mv use_api.py llm_api/use_api.py
  303  ls
  304  touch llm_api/__init__.py
  305  ls
  306  ls ~/compute-boilerplate/config.py 
  307  ln -s ~/compute-boilerplate/config.py /home/nebius/prototyping/llm_api/config.py
  308  ls
  309  ls llm_api/
  310  ls
  311  vim llm_api/use_api.py 
  312  ls
  313  vim llm_api/__init__.py 
  314  exit
  315  ls
  316  cd prototyping/
  317  . ./venv/bin/activate
  318  ls
  319  ls
  320  cd compute-boilerplate/
  321  ls
  322  vim use.py
  323  ls
  324  cd ..
  325  cp compute-boilerplate/use.py prototyping/use_api.py
  326  cd prototyping/
  327  ls
  328  vim use_api.py 
  329  ls
  330  source venv/bin/activate
  331  ls
  332  python3 -m pip install plotly
  333  htop
  334  exit
  335  cd compute-boilerplate/
  336  source venv/bin/activate
  337  python3 main.py
  338  python3 main.py &
  339  htop
  340  ks
  341  ls
  342  cd compute-boilerplate/
  343  ls
  344  cd logs/
  345  ls
  346  vim server.log 
  347  exit
  348  ls
  349  cd compute-boilerplate/
  350  ls
  351  cd logs/
  352  ls
  353  cat server.log 
  354  ls
  355  cat server.log 
  356  cd ..
  357  find 188.121.201.7
  358  find --hel
  359  find --help
  360  clear
  361  ls
  362  exit
  363  htop
  364  exit
  365  ls
  366  cd prototyping/
  367  ls
  368  source venv/bin/activate
  369  ls
  370  cd ..
  371  ls
  372  cd compute-boilerplate/
  373  ls
  374  vim use.py 
  375  cd ..
  376  l
  377  cd prototyping/
  378  ls
  379  cd llm_api/
  380  ls
  381  vim __init__.py 
  382  ls
  383  ps -aux
  384  ls
  385  cd ..
  386  ls
  387  cd compute-boilerplate/
  388  ls
  389  ps -aux | grep python
  390  ls
  391  vim main.py 
  392  clear
  393  exit
  394  ls
  395  cd compute-boilerplate/
  396  ls
  397  cd logs
  398  ls
  399  rm server.log 
  400  touch server.lofgff
  401  exit
  402  ls
  403  cd compute-boilerplate/
  404  ls
  405  cd logs
  406  ls
  407  cat server.log 
  408  vim server.log 
  409  htop
  410  exit
  411  ls
  412  cd compute-boilerplate/
  413  ls
  414  source venv/bin/activate
  415  ls
  416  [A
  417  ls
  418  python3 main.py >> logs/server.log 2>&1
  419  ls
  420  echo "python3 main.py | tee -o  logs/server.log 2>&1 " > 
  421  echo "python3 main.py >> logs/server.log 2>&1" > oldcmd
  422  python3 main.py | tee  logs/server.log
  423  ls
  424  ls
  425  ps -aux | grep 42011
  426  ls
  427  cd compute-boilerplate/
  428  ls
  429  vim main.py 
  430  ls
  431  source venv/bin/activate
  432  which ipython
  433  python -m ipython
  434  python3 -m ipython
  435  python3 -m pip install ipython
  436  python3 -m ipython
  437  ls
  438  source venv/bin/activate
  439  which python
  440  python -m pip install ipython
  441  python -m ipython
  442  which ipython
  443  ipython
  444  ls
  445  vim main.py 
  446  ls
  447  ls
  448  cd flex-llamas-fine-tuning/
  449  ls -lah
  450  cd ..
  451  ls
  452  which python
  453  which python3
  454  ls
  455  python3 -m jupyter notebook --port=42077
  456  python3 -m pip install jupyter
  457  sudo apt-get install python3-piip
  458  sudo apt-get install python3-pip
  459  ls
  460  python3 -m pip install jupyter notebook
  461  ls
  462  python3 -m jupyter notebook --port=42077
  463  python3 -m pip install jupyter-notebook
  464  python3 -m jupyter notebook --port=42077
  465  python3 -m jupyter -h
  466  python3 -m pip install notebook
  467  python3 -m jupyter notebook --port=42077
  468  ls
  469  mkdir prototyping
  470  ls
  471  cd prototyping/
  472  ls
  473  python3 -m venv venv
  474  sudo apt install python3.10-venv
  475  ls
  476  python3 -m venv venv
  477  ls
  478  source venv/bin/activate
  479  alias python=python3
  480  which python3
  481  python3 -m pip install jupyter notebook
  482  python -m jupyter notebook --port=42077
  483  python -m jupyter notebook --port=42011
  484  ls
  485  ps -aux | grep 42011
  486  python -m jupyter notebook --port=42011
  487  l
  488  tmux list-sessions
  489  tmux new-session -t jk2
  490  ls
  491  htop
  492  type -a rg
  493  alias grepr='find . -path \*/venv -prune -or -type f -not -name \*~ -print0 | xargs -0 -r grep '
  494  alias grepri='find . -path \*/venv -prune -or -type f -not -name \*~ -print0 | xargs -0 grep -r -i '
  495  grepr 188.121.201
  496  grepri Vision
  497  type -a dnf
  498  type -a apt
  499  sudo apt-get install ripgrep
  500  type -a rg
  501  rg Vision
  502  ls
  503  find . -name \*Vision\*
  504  tmux a -t llm
  505  ls
  506  cat .bashrc
  507  cat .bash_profile
  508  lss
  509  ls
  510  ls flex-llamas/
  511  cd flex-llamas/
  512  ls
  513  git status
  514  python3 -m venv venv
  515  ls
  516  echo "venv" >> .gitignore
  517  ls
  518  echo ".ipynb_checkpoints" >> .gitignore
  519  git status
  520  ls
  521  source venv/bin/activate
  522  which python
  523  python -m pip install aider
  524  which aider
  525  ls
  526  cd ..
  527  ls
  528  cat .bash_profiel
  529  cat .bash_profile
  530  vim .bashrc
  531  source .bashrc
  532  ls
  533  cd flex-llamas/
  534  source venv/bin/activate
  535  ls
  536  tmux list-sessions
  537  ls
  538  cd ..
  539  ls
  540  cd prototyping/
  541  ls
  542  source venv/bin/activate
  543  ls
  544  tmux list-sessions
  545  tmux a
  546  exit
  547  htop
  548  ps
  549  ps -a
  550  cd compute-boilerplate/
  551  source venv/bin/activate
  552  python3 main.py
  553  python3 main.py &
  554  git clone git@github.com:cerebralvalley/compute-boilerplate.git
  555  python3 -m venv compute-boilerplate/
  556  cd compute-boilerplate/
  557  ls
  558  source bin/activate
  559  pip3 install -r requirements.txt 
  560  vim config.py 
  561  curl http://localhost:42069/api/generate -d '{
  562    "model": "llama3.2",
  563    "prompt":"Why is the sky blue?"
  564  }'
  565  ls
  566  cd prototyping/
  567  ls
  568  source venv/bin/activate
  569  python3 -m pip install openai
  570  ls
  571  cd llm_api/
  572  ls
  573  vim use_api.py 
  574  curl -fsSL https://ollama.com/install.sh | sh
  575  ollama run llama3.2
  576  tmux ls
  577  tmux new -s brit
  578  exit
  579  ollama run llama3.2
  580  ollama list
  581  ollama --help
  582  ollama run llama3.2:1b
  583  ls
  584  which ollama
  585  ollama
  586  ollama serve llama3.2
  587  ollama serve
  588  exit
  589  OLLAMA_HOST=127.0.0.1:42069 ollama serve
  590  OLLAMA_HOST=127.0.0.1:8000 ollama serve
  591  ollama list
  592  OLLAMA_HOST=127.0.0.1:8000 ollama serve
  593  OLLAMA_HOST=127.0.0.1:42069 ollama serve
  594  OLLAMA_HOST=127.0.0.1:8000 ollama serve
  595  exit
  596  ls
  597  curl http://localhost:42069/api/generate -d '{
  598    "model": "llama3.2",
  599    "prompt":"Why is the sky blue?"
  600  }'
  601  curl http://localhost:42069/api/generate -d '{
  602    "model": "llama3.2",
  603    "prompt":"Why is the sky blue?"
  604  }'
  605  curl http://localhost:42069/api/generate -d '{
  606    "model": "llama3.2",
  607    "prompt":"Why is the sky blue?"
  608  }'
  609  curl http://localhost:42069/api/generate -d '{
  610    "model": "llama3.2",
  611    "prompt":"Why is the sky blue?"
  612  }'
  613  curl http://localhost:42069/v1/chat/completions     -H "Content-Type: application/json"     -d '{
  614          "model": "llama3.2",
  615          "messages": [
  616              {
  617                  "role": "system",
  618                  "content": "You are a helpful assistant."
  619              },
  620              {
  621                  "role": "user",
  622                  "content": "Hello!"
  623              }
  624          ]
  625      }'
  626  ollama list
  627  curl http://localhost:42069/v1/chat/completions     -H "Content-Type: application/json"     -d '{
  628          "model": "llama3.2",
  629          "messages": [
  630              {
  631                  "role": "system",
  632                  "content": "You are a helpful assistant."
  633              },
  634              {
  635                  "role": "user",
  636                  "content": "Hello!"
  637              }
  638          ]
  639      }'
  640  curl http://localhost:42069/v1/chat/completions     -H "Content-Type: application/json"     -d '{
  641          "model": "llama3.2:latest",
  642          "messages": [
  643              {
  644                  "role": "system",
  645                  "content": "You are a helpful assistant."
  646              },
  647              {
  648                  "role": "user",
  649                  "content": "Hello!"
  650              }
  651          ]
  652      }'
  653  curl http://localhost:42069/v1/chat/completions     -H "Content-Type: application/json"     -d '{
  654          "model": "llama3.2:latest",
  655          "messages": [
  656              {
  657                  "role": "system",
  658                  "content": "You are a helpful assistant."
  659              },
  660              {
  661                  "role": "user",
  662                  "content": "Hello!"
  663              }
  664          ]
  665      }'
  666  curl http://localhost:42069/v1/chat/completions     -H "Content-Type: application/json"     -d '{
  667          "model": "llama3.2:latest",
  668          "messages": [
  669              {
  670                  "role": "system",
  671                  "content": "You are a helpful assistant."
  672              },
  673              {
  674                  "role": "user",
  675                  "content": "Hello!"
  676              }
  677          ]
  678      }'
  679  curl http://localhost:42069/v1/chat/completions     -H "Content-Type: application/json"     -d '{
  680          "model": "llama",
  681          "messages": [
  682              {
  683                  "role": "system",
  684                  "content": "You are a helpful assistant."
  685              },
  686              {
  687                  "role": "user",
  688                  "content": "Hello!"
  689              }
  690          ]
  691      }'
  692  exit
  693  ollama serve
  694  man ollama
  695  ollama -h
  696  ollama serve -h
  697  OLLAMA_HOST=127.0.0.1:42069 ollama serve
  698  ollama pull llama3.2
  699  OLLAMA_HOST=127.0.0.1:42069 ollama serve
  700  curl http://localhost:8000/v1/chat/completions     -H "Content-Type: application/json"     -d '{
  701          "model": "llama",
  702          "messages": [
  703              {
  704                  "role": "system",
  705                  "content": "You are a helpful assistant."
  706              },
  707              {
  708                  "role": "user",
  709                  "content": "Hello!"
  710              }
  711          ]
  712      }'
  713  curl http://localhost:8000/v1/chat/completions     -H "Content-Type: application/json"     -d '{
  714          "model": "llama3.2",
  715          "messages": [
  716              {
  717                  "role": "system",
  718                  "content": "You are a helpful assistant."
  719              },
  720              {
  721                  "role": "user",
  722                  "content": "Hello!"
  723              }
  724          ]
  725      }'
  726  curl http://localhost:8000/v1/chat/completions     -H "Content-Type: application/json"     -d '{
  727          "model": "llama3.2:1b",
  728          "messages": [
  729              {
  730                  "role": "system",
  731                  "content": "You are a helpful assistant."
  732              },
  733              {
  734                  "role": "user",
  735                  "content": "Hello!"
  736              }
  737          ]
  738      }'
  739  exit
  740  ollama 
  741  ollama run
  742  ollama run llama3.2
  743  exit
  744  ps
  745  ps -a
  746  cd compute-boilerplate/
  747  source venv/bin/activate
  748  ls
  749  python3 main.py
  750  top
  751  netstat -nlp | grep 8000
  752  apt install net-tools
  753  sdo apt install net-tools
  754  sudo apt install net-tools
  755  netstat -nlp | grep 8000
  756  kill --help
  757  kill 93868
  758  netstat -nlp | grep 8000
  759  sudo netstat -nlp | grep 8000
  760  ls
  761  tmux list-sessions
  762  tmux a jk2
  763  tmux a -t jk2
  764  exit
  765  cd prototyping/
  766  ls
  767  source venv/bin/
  768  source venv/bin/activate
  769  python3 -m jupyter notebook --port=42011
  770  OLLAMA_HOST=127.0.0.1:8000 ollama serve
  771  ls
  772  cd prototyping/
  773  ls
  774  source venv/bin/activate
  775  curl http://localhost:8000/v1/chat/completions     -H "Content-Type: application/json"     -d '{
  776          "model": "llama3.2",
  777          "messages": [
  778              {
  779                  "role": "system",
  780                  "content": "You are a helpful assistant."
  781              },
  782              {
  783                  "role": "user",
  784                  "content": "Hello!"
  785              }
  786          ]
  787      }'
  788  ollama list
  789  which ollama
  790  ls -l /usr/local/bin/ollama 
  791  clear
  792  systemctl --user oll
  793  systemctl --user status ollama
  794  systemctl status ollama
  795  reboot
  796  sudo reboot
  797  ks
  798  ls
  799  cp -p jk_synth.ipynb martin.ipynb
  800  python -m jupyter notebook
  801  . ./venv/bin/activate
  802  python -m jupyter notebook
  803  tmux ls
  804  man tmux
  805  tmux attach brit
  806  tmux attach -t brit
  807  tmux new -s britannio
  808  exit
  809  ls
  810  tmux list-sessions
  811  tmux new-session -t jamesk
  812  exit
  813  ls
  814  exit
  815  ls
  816  cd prototyping/
  817  ls
  818  source venv/bin/activate
  819  ipython
  820  ls
  821  ls
  822  ls
  823  cd prototyping/
  824  ls
  825  clear
  826  ps -aux | grep 42011
  827  ls
  828  curl http://localhost:42069/v1/chat/completions     -H "Content-Type: application/json"     -d '{
  829          "model": "llama3.2",
  830          "messages": [
  831              {
  832                  "role": "system",
  833                  "content": "You are a helpful assistant."
  834              },
  835              {
  836                  "role": "user",
  837                  "content": "Hello!"
  838              }
  839          ]
  840      }'
  841  ollama list
  842  sudo systemctl status ollama
  843  curl http://127.0.0.1:42069/v1/chat/completions     -H "Content-Type: application/json"     -d '{
  844          "model": "llama3.2",
  845          "messages": [
  846              {
  847                  "role": "system",
  848                  "content": "You are a helpful assistant."
  849              },
  850              {
  851                  "role": "user",
  852                  "content": "Hello!"
  853              }
  854          ]
  855      }'
  856  ollama run mistral
  857  curl http://127.0.0.1:42069/v1/chat/completions     -H "Content-Type: application/json"     -d '{
  858          "model": "mistral",
  859          "messages": [
  860              {
  861                  "role": "system",
  862                  "content": "You are a helpful assistant."
  863              },
  864              {
  865                  "role": "user",
  866                  "content": "Hello!"
  867              }
  868          ]
  869      }'
  870  curl http://127.0.0.1:42069/v1/chat/completions     -H "Content-Type: application/json"     -d '{
  871          "model": "mistral",
  872          "messages": [
  873              {
  874                  "role": "system",
  875                  "content": "You are a helpful assistant."
  876              },
  877              {
  878                  "role": "user",
  879                  "content": "Hello!"
  880              }
  881          ]
  882      }'
  883  curl http://127.0.0.1:42069/v1/chat/completions     -H "Content-Type: application/json"     -d '{
  884          "model": "llama3.2",
  885          "messages": [
  886              {
  887                  "role": "system",
  888                  "content": "You are a helpful assistant."
  889              },
  890              {
  891                  "role": "user",
  892                  "content": "Hello!"
  893              }
  894          ]
  895      }'
  896  curl localhost:42069
  897  ollama show llama3.2
  898  clear
  899  ls
  900  tmux list-sessions
  901  tmux a -t jamesk
  902  tmux a -s jamesk
  903  tmux new-session -s jamesk
  904  tmux a -s jamesk
  905  tmux a -t jamesk
  906  ls
  907  edgenode -t calcutta -t wsl
  908  exit
  909  ls
  910  cd prototyping/
  911  ls
  912  vim groq_apikey 
  913  ls /home/nebius/.ollama/models
  914  ls /home/nebius/.ollama/models/blobs
  915  ls /home/nebius/.ollama/models -lah
  916  ls /home/nebius/.ollama/models/blobs -lah
  917  ls /home/nebius/.ollama/models -lah
  918  ls
  919  find / -name ollama3.2:latest
  920  sudo find / -name ollama3.2:latest
  921  ls /usr
  922  ls /usr/et
  923  ls /usr/etc
  924  ls /usr/share
  925  ls /usr/local
  926  ls /usr/local/bin
  927  ls /usr/local/sbin
  928  ls /usr/local/share
  929  ollama list
  930  exit
  931  sudo lsof -i -P -n | grep LISTEN.
  932  ls
  933  cd prototyping/
  934  ls
  935  clear
  936  ls
  937  source venv/bin/activate
  938  which aider
  939  python -m aider
  940  python3 -m aider
  941  python3 -m pip install aider
  942  which aider
  943  source venv/bin/activate
  944  which aider
  945  python3 -m aider
  946  which aider
  947  ls
  948  ls venv
  949  ls venv/bin
  950  find venv -f aider
  951  find venv aider
  952  find -h
  953  find --help
  954  find venv -name aider
  955  which aider
  956  ls
  957  aider
  958  which aider
  959  unsource
  960  desource
  961  source ~/.bashrc
  962  ls
  963  aider
  964  ls
  965  ls venv/li
  966  ls venv/lib
  967  ls venv/lib/python3.10/
  968  ls venv/lib/python3.10/site-packages/
  969  find venv --name aider
  970  find venv -name aider
  971  ls venv/lib/python3.10/site-packages/aider
  972  ls venv/lib/python3.10/site-packages/aider/__init__.py
  973  cat venv/lib/python3.10/site-packages/aider/__init__.py
  974  python3 -m pip uninstall aider
  975  ls
  976  source venv/bin/activate
  977  python3 -m pip install aider
  978  ls
  979  rm -rf ./venv/lib/python3.10/site-packages/aider
  980  python3 -m pip install aider
  981  python -m aider
  982  python -m pip install aider
  983  python -m pip install aider --force
  984  python -m pip install aider
  985  python -m aider
  986  ls
  987  vim extremely_bad_tui.py 
  988  rm extremely_bad_tui.py 
  989  vim bad_tui.py
  990  cat ~/.bashrc
  991  ls
  992  vim bad_tui.py 
  993  vim bad_tui.py
  994  python3 bad_tui.py 
  995  python3 -m pip install anthropic
  996  python3 bad_tui.py 
  997  vim bad_tui.py
  998  python3 bad_tui.py 
  999  vim bad_tui.py
 1000  python3 bad_tui.py 
 1001  vim bad_tui.py
 1002  python3 bad_tui.py 
 1003  vim bad_tui.py
 1004  python3 bad_tui.py 
 1005  vim bad_tui.py
 1006  cat ~/.bashrc
 1007  vim bad_tui.py
 1008  python3 bad_tui.py 
 1009  vim bad_tui.py
 1010  python3 bad_tui.py 
 1011  python -m pip install groq
 1012  vim bad_tui.py
 1013  python3 bad_tui.py 
 1014  vim bad_tui.py
 1015  python3 bad_tui.py 
 1016  vim bad_tui.py
 1017  python3 bad_tui.py 
 1018  vim bad_tui.py
 1019  which ipython
 1020  ipython
 1021  ls
 1022  vim bad_tui.py 
 1023  ipython
 1024  ls
 1025  vim bad_tui.py 
 1026  python3 bad_tui.py
 1027  vim bad_tui.py 
 1028  python3 bad_tui.py
 1029  vim bad_tui.py 
 1030  python3 bad_tui.py
 1031  vim bad_tui.py 
 1032  python3 bad_tui.py
 1033  vim bad_tui.py 
 1034  ls
 1035  vim test
 1036  ps -aux | grep ollama
 1037  ps -aux | grep "ollama serve"
 1038  ps -aux | grep "ollama\sserve"
 1039  which ollama
 1040  vim test
 1041  ls
 1042  vim test
 1043  crontab -e
 1044  sudo crontab -e
 1045  ps -aux | grep "ollama\sserve"
 1046  ps kill 25015
 1047  sudo crontab -e
 1048  ps -aux | grep "ollama\sserve"
 1049  kill 25015
 1050  sudo kill 25015
 1051  ps -aux | grep "ollama\sserve"
 1052  netstat -tuln
 1053  [200~sudo fuser -n tcp -v
 1054  ~
 1055  sudo fuser -n tcp -v
 1056  sudo fuser 80/tcp
 1057  sudo fuser */tcp
 1058  sudo fuser 42069/tcp
 1059  sudo crontab -e
 1060  ls
 1061  ps -aux | grep "ollama\sserve"
 1062  kill 31366
 1063  sudo kill 31366
 1064  ps -aux | grep "ollama\sserve"
 1065  sudo crontab -e
 1066  ls
 1067  ps -aux | grep "ollama\sserve"
 1068  sudo kill 31969
 1069  cd prototyping/
 1070  d
 1071  ls
 1072  cp jk_synth.ipynb martin2.ipynb
 1073  ls
 1074  cd ..
 1075  ls
 1076  mkdir martin-tmp.txt
 1077  mv martin-tmp.txt martin
 1078  cd martin
 1079  history > hist.txt
